{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3572572",
   "metadata": {},
   "source": [
    "Version 5 of reproduction of the following paper : Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "\n",
    "What is added from V4 : \n",
    "- Based on LLMware, but applied to TriviaQA --> application to something else so that the learning sticks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9a638",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97de868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.setup import Setup\n",
    "from llmware.status import Status\n",
    "from llmware.models import ModelCatalog\n",
    "from llmware.configs import LLMWareConfig, MilvusConfig\n",
    "\n",
    "import time\n",
    "from llmware.prompts import Prompt, HumanInTheLoop\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "from importlib import util\n",
    "\n",
    "from datasets import Dataset, load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591db95",
   "metadata": {},
   "source": [
    "LIBRARY : import documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2bffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': Value('string'),\n",
       " 'question_id': Value('string'),\n",
       " 'question_source': Value('string'),\n",
       " 'entity_pages': {'doc_source': List(Value('string')),\n",
       "  'filename': List(Value('string')),\n",
       "  'title': List(Value('string')),\n",
       "  'wiki_context': List(Value('string'))},\n",
       " 'search_results': {'description': List(Value('string')),\n",
       "  'filename': List(Value('string')),\n",
       "  'rank': List(Value('int32')),\n",
       "  'title': List(Value('string')),\n",
       "  'url': List(Value('string')),\n",
       "  'search_context': List(Value('string'))},\n",
       " 'answer': {'aliases': List(Value('string')),\n",
       "  'normalized_aliases': List(Value('string')),\n",
       "  'matched_wiki_entity_name': Value('string'),\n",
       "  'normalized_matched_wiki_entity_name': Value('string'),\n",
       "  'normalized_value': Value('string'),\n",
       "  'type': Value('string'),\n",
       "  'value': Value('string')}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT TRIVIAQA dataset\n",
    "\n",
    "# Load TriviaQA (unfiltered for simplicity)\n",
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\")\n",
    "\n",
    "# Take first 50 examples for quick testing\n",
    "test_set = dataset[\"train\"].select(range(50))\n",
    "\n",
    "# List of the features of the dataset\n",
    "features = dataset[\"train\"].features; features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1998 was the Chinese year of which creature?\n",
      "Answer: Tiger\n"
     ]
    }
   ],
   "source": [
    "#PRINT ONE QUESTION AND ANSWER\n",
    "\n",
    "digit = 77\n",
    "\n",
    "example = dataset[\"train\"][digit]\n",
    "\n",
    " \n",
    "\n",
    "# Print the question and its answer\n",
    "print(\"Question:\", example[\"question\"])\n",
    "print(\"Answer:\", example[\"answer\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7637a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION THAT WILL CREATE A LIBRARY\n",
    "\n",
    "def create_library(library_name):\n",
    "\n",
    "    print (f\"\\n > Creating library '{library_name}'...\")\n",
    "\n",
    "    library = Library().create_new_library(library_name)\n",
    "\n",
    "    return library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd04af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Creating library 'RAG_V5_Lib'...\n"
     ]
    }
   ],
   "source": [
    "# CREATE LIBRARY\n",
    "\n",
    "library_name = \"RAG_V5_Lib\"\n",
    "library = create_library(library_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f159b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO PARSE THE TRIVIAQA DATASET AND ADD THEM TO THE LIBRARY\n",
    "\n",
    "def dataset_to_file(dataset):\n",
    "    print(\"\\n > Transferring dataset to file ...\")\n",
    "\n",
    "    # Extract text from dataset (e.g., questions and answers)\n",
    "    texts = []\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(\"Data\", exist_ok=True)\n",
    "\n",
    "\n",
    "    for i in range(len(dataset[\"train\"][1:10])):  # Limiting to first 10 for demonstration\n",
    "        question = dataset[\"train\"][\"question\"][i]\n",
    "        answer = dataset[\"train\"][\"answer\"][i]\n",
    "        combined_text = f\"Q: {question}\\nA: {answer}\"\n",
    "        #texts.append(combined_text)\n",
    "\n",
    "        # Define the full path to the file\n",
    "        file_path = os.path.join(\"Data\", f\"TriviaQA_{i}.txt\")\n",
    "\n",
    "        # Write the combined text to a file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(combined_text)\n",
    "            print(f\"File saved as {file_path}\")\n",
    "\n",
    "def parse_files(library, data_path):\n",
    "    print (f\"\\n > Parsing and adding dataset to library ...\")\n",
    "    library.add_files(input_folder_path=data_path, chunk_size=400, max_chunk_size=800, smart_chunking=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0a507eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Transferring dataset to file ...\n",
      "File saved as Data\\TriviaQA_0.txt\n",
      "File saved as Data\\TriviaQA_1.txt\n",
      "File saved as Data\\TriviaQA_2.txt\n",
      "File saved as Data\\TriviaQA_3.txt\n",
      "File saved as Data\\TriviaQA_4.txt\n",
      "File saved as Data\\TriviaQA_5.txt\n",
      "\n",
      " > Parsing and adding dataset to library ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO: update:  Duplicate files (skipped): 0\u001b[39m\n",
      "\u001b[37mINFO: update:  Total uploaded: 6\u001b[39m\n",
      "\u001b[37mINFO: Parser - parse_text file - processing - TriviaQA_0.txt\u001b[39m\n",
      "\u001b[37mINFO: Parser - parse_text file - processing - TriviaQA_1.txt\u001b[39m\n",
      "\u001b[37mINFO: Parser - parse_text file - processing - TriviaQA_2.txt\u001b[39m\n",
      "\u001b[37mINFO: Parser - parse_text file - processing - TriviaQA_3.txt\u001b[39m\n",
      "\u001b[37mINFO: Parser - parse_text file - processing - TriviaQA_4.txt\u001b[39m\n",
      "\u001b[37mINFO: Parser - parse_text file - processing - TriviaQA_5.txt\u001b[39m\n"
     ]
    }
   ],
   "source": [
    " #APPLY THE FUNCTION\n",
    "dataset_to_file(dataset)\n",
    "parse_files(library, \"Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56999a",
   "metadata": {},
   "source": [
    "RETRIEVER EMBEDDINGS : Turn text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d55578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO MAKE EMBEDDINGS\n",
    "\n",
    "def make_embeddings(embedding_model_name, vector_db):\n",
    "    print(\"Generating Embeddings in {} db - with Model- {}\".format(vector_db, embedding_model_name))\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "    MilvusConfig().set_config(\"lite\", True)\n",
    "    LLMWareConfig().set_vector_db(vector_db)\n",
    "    library.install_new_embedding(embedding_model_name=embedding_model_name, vector_db=vector_db, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae8bfa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all-MiniLM-L6-v2',\n",
       " 'all-mpnet-base-v2',\n",
       " 'industry-bert-insurance',\n",
       " 'industry-bert-contracts',\n",
       " 'industry-bert-asset-management',\n",
       " 'industry-bert-sec',\n",
       " 'industry-bert-loans',\n",
       " 'nomic-ai/nomic-embed-text-v1',\n",
       " 'jinaai/jina-embeddings-v2-base-en',\n",
       " 'jinaai/jina-embeddings-v2-small-en',\n",
       " 'BAAI/bge-small-en-v1.5',\n",
       " 'BAAI/bge-large-en-v1.5',\n",
       " 'BAAI/bge-base-en-v1.5',\n",
       " 'thenlper/gte-small',\n",
       " 'thenlper/gte-base',\n",
       " 'thenlper/gte-large',\n",
       " 'llmrails/ember-v1',\n",
       " 'WhereIsAI/UAE-Large-V1',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-embedding-3-small',\n",
       " 'text-embedding-3-large',\n",
       " 'medium',\n",
       " 'xlarge',\n",
       " 'embed-english-v3.0',\n",
       " 'embed-multilingual-v3.0',\n",
       " 'embed-english-light-v3.0',\n",
       " 'embed-multilingual-light-v3.0',\n",
       " 'embed-english-v2.0',\n",
       " 'embed-english-light-v2.0',\n",
       " 'embed-multilingual-v2.0',\n",
       " 'textembedding-gecko@latest']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIST OF THE EMBEDDING MODELS AVAILABLE\n",
    "embedding_models = ModelCatalog().list_embedding_models()\n",
    "model_names = [model['model_name'] for model in embedding_models]; model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "794142a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings in faiss db - with Model- mini-lm-sbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO: update: embedding_handler - FAISS - Embeddings Created: 7 of 7\u001b[39m\n",
      "\u001b[37mINFO: update: EmbeddingHandler - FAISS - embedding_summary - {'embeddings_created': 7, 'embedded_blocks': 22, 'embedding_dims': 384, 'time_stamp': '2025-11-12_202054'}\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# MAKE EMBEDDINGS\n",
    "embedding_model_name = \"mini-lm-sbert\"\n",
    "vector_db = \"faiss\"\n",
    "make_embeddings(embedding_model_name, vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6310cf4",
   "metadata": {},
   "source": [
    "GENERATOR : PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78184b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO MAKE GENERATOR MODEL\n",
    "def generator(llm_model_name, digit):\n",
    "    print(\"Loading model for LLM inference - \", llm_model_name)\n",
    "    query = dataset[\"train\"][digit][\"question\"]\n",
    "    prompter = Prompt().load_model(llm_model_name, temperature=0.0, sample=False)\n",
    "    results = Query(library).semantic_query(query, result_count=80, embedding_distance_threshold=1.0)\n",
    "\n",
    "    # for each document in the library, we will run a query and look at the results\n",
    "\n",
    "    for i, contract in enumerate(os.listdir(\"Data\")):\n",
    "        qr = []\n",
    "        if contract != \".DS_Store\":\n",
    "            print(\"\\nContract Name: \", i, contract)\n",
    "\n",
    "            #   we will look through the list of semantic query results, and pull the top results for each file\n",
    "            for j, entries in enumerate(results):\n",
    "                library_fn = entries[\"file_source\"]\n",
    "\n",
    "                if os.sep in library_fn:\n",
    "                    # handles difference in windows file formats vs. mac / linux\n",
    "                    library_fn = library_fn.split(os.sep)[-1]\n",
    "\n",
    "                if library_fn == contract:\n",
    "                   # print(\"Top Retrieval: \", j, entries[\"distance\"], entries[\"text\"])\n",
    "                    qr.append(entries)\n",
    "\n",
    "            #   we will add the query results to the prompt\n",
    "            source = prompter.add_source_query_results(query_results=qr)\n",
    "\n",
    "            #   run the prompt\n",
    "            response = prompter.prompt_with_source(query, prompt_name=\"default_with_context\")\n",
    "\n",
    "            #   note: prompt_with_resource returns a list of dictionary responses\n",
    "            #   -- depending upon the size of the source context, it may call the llm several times\n",
    "            #   -- each dict entry represents 1 call to the LLM\n",
    "\n",
    "            #   post processing fact checking\n",
    "            answer = dataset[\"train\"][digit][\"answer\"][\"value\"]\n",
    "\n",
    "            print(\"\\nupdate: llm answer - \", response[i][\"llm_response\"])\n",
    "            print(\"update: Right answer - \", answer[i])\n",
    "\n",
    "            # start fresh for next document\n",
    "            prompter.clear_source_materials()\n",
    "\n",
    "            # Save jsonl report with full transaction history to /prompt_history folder\n",
    "            print(\"\\nupdate: Prompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n",
    "\n",
    "            prompter.save_state()\n",
    "\n",
    "            # Generate CSV report for easy Human review in Excel\n",
    "            csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "            print(\"\\nupdate: CSV output for human review - \", csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6747375e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meta-Llama-3-8B',\n",
       " 'Meta-Llama-3-8B-Instruct',\n",
       " 'QuantFactory/Meta-Llama-3-8B-GGUF',\n",
       " 'QuantFactory/Meta-Llama-3-8B-Instruct-GGUF',\n",
       " 'TheBloke/Llama-2-7B-Chat-GGUF',\n",
       " 'TheBloke/OpenHermes-2.5-Mistral-7B-GGUF',\n",
       " 'TheBloke/Starling-LM-7B-alpha-GGUF',\n",
       " 'TheBloke/zephyr-7B-beta-GGUF',\n",
       " 'bartowski/Meta-Llama-3-8B-Instruct-GGUF',\n",
       " 'bling-answer-tool',\n",
       " 'bling-phi-2-gguf',\n",
       " 'bling-phi-3-gguf',\n",
       " 'bling-phi-3-onnx',\n",
       " 'bling-phi-3-ov',\n",
       " 'bling-phi-3.5-gguf',\n",
       " 'bling-qwen-0.5b-gguf',\n",
       " 'bling-qwen-1.5b-gguf',\n",
       " 'bling-qwen-1.5b-ov',\n",
       " 'bling-qwen-500m-ov',\n",
       " 'bling-stablelm-3b-tool',\n",
       " 'bling-tiny-llama-onnx',\n",
       " 'bling-tiny-llama-ov',\n",
       " 'chat-bison@001',\n",
       " 'claude-2.0',\n",
       " 'claude-2.1',\n",
       " 'claude-3-5-haiku-20241022',\n",
       " 'claude-3-5-sonnet-20240620',\n",
       " 'claude-3-7-sonnet-20250219',\n",
       " 'claude-3-haiku-20240307',\n",
       " 'claude-3-opus-20240229',\n",
       " 'claude-3-sonnet-20240229',\n",
       " 'claude-instant-v1',\n",
       " 'claude-v1',\n",
       " 'codegemma-7b-it-ov',\n",
       " 'command-medium-nightly',\n",
       " 'command-xlarge-nightly',\n",
       " 'deepseek-qwen-14b-gguf',\n",
       " 'deepseek-qwen-7b-gguf',\n",
       " 'dolphin-2.9.3-mistral-7b-32k-ov',\n",
       " 'dolphin-2.9.4-llama3.1-8b-ov',\n",
       " 'dragon-llama-3.1-gguf',\n",
       " 'dragon-llama-answer-tool',\n",
       " 'dragon-llama2-ov',\n",
       " 'dragon-mistral-0.3-gguf',\n",
       " 'dragon-mistral-0.3-onnx',\n",
       " 'dragon-mistral-0.3-ov',\n",
       " 'dragon-mistral-answer-tool',\n",
       " 'dragon-mistral-ov',\n",
       " 'dragon-qwen-7b-gguf',\n",
       " 'dragon-qwen-7b-ov',\n",
       " 'dragon-yi-6b-ov',\n",
       " 'dragon-yi-9b-gguf',\n",
       " 'dragon-yi-9b-ov',\n",
       " 'dragon-yi-9b-ov',\n",
       " 'dragon-yi-answer-tool',\n",
       " 'dreamgen-wizardlm-2-7b-ov',\n",
       " 'gemini-1.5-pro',\n",
       " 'gemma-2-27b-instruct-gguf',\n",
       " 'gemma-2-9b-instruct-gguf',\n",
       " 'gemma-2b-it-onnx',\n",
       " 'gemma-2b-it-ov',\n",
       " 'gemma-7b-it-ov',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'gpt-3.5-turbo-instruct',\n",
       " 'gpt-4',\n",
       " 'gpt-4-0125-preview',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'gpt-4o-2024-08-06',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-mini-2024-07-18',\n",
       " 'intel-neural-chat-7b-v3-2-ov',\n",
       " 'j2-grande-instruct',\n",
       " 'j2-jumbo-instruct',\n",
       " 'llama-2-13b-chat-ov',\n",
       " 'llama-2-chat-onnx',\n",
       " 'llama-2-chat-ov',\n",
       " 'llama-3.1-instruct-gguf',\n",
       " 'llama-3.1-instruct-onnx',\n",
       " 'llama-3.1-instruct-ov',\n",
       " 'llama-3.2-1b-instruct-gguf',\n",
       " 'llama-3.2-1b-instruct-onnx',\n",
       " 'llama-3.2-1b-instruct-ov',\n",
       " 'llama-3.2-3b-instruct-gguf',\n",
       " 'llama-3.2-3b-instruct-onnx',\n",
       " 'llama-3.2-3b-instruct-ov',\n",
       " 'llama-3.2-3b-onnx-qnn',\n",
       " 'llmware-inference-server',\n",
       " 'llmware/bling-1.4b-0.1',\n",
       " 'llmware/bling-1b-0.1',\n",
       " 'llmware/bling-cerebras-1.3b-0.1',\n",
       " 'llmware/bling-falcon-1b-0.1',\n",
       " 'llmware/bling-phi-3',\n",
       " 'llmware/bling-phi-3.5',\n",
       " 'llmware/bling-red-pajamas-3b-0.1',\n",
       " 'llmware/bling-sheared-llama-1.3b-0.1',\n",
       " 'llmware/bling-sheared-llama-2.7b-0.1',\n",
       " 'llmware/bling-stable-lm-3b-4e1t-v0',\n",
       " 'llmware/bling-tiny-llama-v0',\n",
       " 'llmware/dragon-deci-6b-v0',\n",
       " 'llmware/dragon-deci-7b-v0',\n",
       " 'llmware/dragon-falcon-7b-v0',\n",
       " 'llmware/dragon-llama-3.1',\n",
       " 'llmware/dragon-llama-7b-gguf',\n",
       " 'llmware/dragon-llama-7b-v0',\n",
       " 'llmware/dragon-mistral-0.3',\n",
       " 'llmware/dragon-mistral-7b-gguf',\n",
       " 'llmware/dragon-mistral-7b-v0',\n",
       " 'llmware/dragon-qwen-7b',\n",
       " 'llmware/dragon-red-pajama-7b-v0',\n",
       " 'llmware/dragon-stablelm-7b-v0',\n",
       " 'llmware/dragon-yi-6b-gguf',\n",
       " 'llmware/dragon-yi-6b-v0',\n",
       " 'llmware/slim-category',\n",
       " 'llmware/slim-emotions',\n",
       " 'llmware/slim-extract-tiny-tool',\n",
       " 'llmware/slim-intent',\n",
       " 'llmware/slim-ner',\n",
       " 'llmware/slim-nli',\n",
       " 'llmware/slim-q-gen-phi-3',\n",
       " 'llmware/slim-q-gen-tiny',\n",
       " 'llmware/slim-qa-gen-phi-3',\n",
       " 'llmware/slim-qa-gen-tiny',\n",
       " 'llmware/slim-ratings',\n",
       " 'llmware/slim-sentiment',\n",
       " 'llmware/slim-sql-1b-v0',\n",
       " 'llmware/slim-summary-tiny-tool',\n",
       " 'llmware/slim-tags',\n",
       " 'llmware/slim-topics',\n",
       " 'mathstral-7b-ov',\n",
       " 'microsoft/Phi-3-mini-128k-instruct',\n",
       " 'microsoft/Phi-3-mini-4k-instruct',\n",
       " 'microsoft/Phi-3-mini-4k-instruct-gguf',\n",
       " 'mistral-7b-instruct-v0.2-ov',\n",
       " 'mistral-7b-instruct-v0.3-gguf',\n",
       " 'mistral-7b-instruct-v0.3-onnx',\n",
       " 'mistral-7b-instruct-v0.3-ov',\n",
       " 'mistral-7b-instruct-v0.3-ov',\n",
       " 'mistral-nemo-instruct-2407-ov',\n",
       " 'mistral-small-instruct-2409-ov',\n",
       " 'nvidia-llama3-chatqa-1.5-8b-ov',\n",
       " 'o1',\n",
       " 'o1-pro',\n",
       " 'o3-mini',\n",
       " 'openchat-3.6-8b-20240522-ov',\n",
       " 'phi-3-onnx',\n",
       " 'phi-3-onnx',\n",
       " 'phi-3-ov',\n",
       " 'phi-3.5-gguf',\n",
       " 'phi-4-gguf',\n",
       " 'phi-4-mini-gguf',\n",
       " 'qwen-2.5-14b-instruct-gguf',\n",
       " 'qwen-2.5-7b-coder-gguf',\n",
       " 'qwen2-0.5b-chat-ov',\n",
       " 'qwen2-0.5b-instruct-gguf',\n",
       " 'qwen2-0.5b-instruct-gguf',\n",
       " 'qwen2-1.5b-instruct-gguf',\n",
       " 'qwen2-1.5b-instruct-gguf',\n",
       " 'qwen2-1.5b-instruct-ov',\n",
       " 'qwen2-7B-instruct-gguf',\n",
       " 'qwen2-7B-instruct-gguf',\n",
       " 'qwen2-7b-instruct-ov',\n",
       " 'qwen2.5-0.5b-instruct-ov',\n",
       " 'qwen2.5-0.5b-instruct-ov',\n",
       " 'qwen2.5-1.5b-instruct-ov',\n",
       " 'qwen2.5-14b-instruct-ov',\n",
       " 'qwen2.5-32b-gguf',\n",
       " 'qwen2.5-3b-instruct-ov',\n",
       " 'qwen2.5-3b-instruct-ov',\n",
       " 'qwen2.5-7b-instruct-ov',\n",
       " 'qwen2.5-coder-7b-instruct-ov',\n",
       " 'slim-boolean',\n",
       " 'slim-boolean-phi-3-gguf',\n",
       " 'slim-boolean-phi-3-onnx',\n",
       " 'slim-boolean-phi-3-ov',\n",
       " 'slim-boolean-tool',\n",
       " 'slim-category-tool',\n",
       " 'slim-emotions-onnx',\n",
       " 'slim-emotions-ov',\n",
       " 'slim-emotions-tool',\n",
       " 'slim-extract',\n",
       " 'slim-extract-phi-3-gguf',\n",
       " 'slim-extract-phi-3-onnx',\n",
       " 'slim-extract-phi-3-ov',\n",
       " 'slim-extract-qwen-0.5b-ov',\n",
       " 'slim-extract-qwen-1.5b-gguf',\n",
       " 'slim-extract-qwen-1.5b-ov',\n",
       " 'slim-extract-qwen-nano-gguf',\n",
       " 'slim-extract-tiny',\n",
       " 'slim-extract-tiny-onnx',\n",
       " 'slim-extract-tiny-ov',\n",
       " 'slim-extract-tool',\n",
       " 'slim-intent-onnx',\n",
       " 'slim-intent-ov',\n",
       " 'slim-intent-tool',\n",
       " 'slim-ner-onnx',\n",
       " 'slim-ner-ov',\n",
       " 'slim-ner-tool',\n",
       " 'slim-nli-tool',\n",
       " 'slim-q-gen-phi-3-tool',\n",
       " 'slim-q-gen-tiny-ov',\n",
       " 'slim-q-gen-tiny-tool',\n",
       " 'slim-qa-gen-phi-3-tool',\n",
       " 'slim-qa-gen-tiny-ov',\n",
       " 'slim-qa-gen-tiny-tool',\n",
       " 'slim-ratings-onnx',\n",
       " 'slim-ratings-ov',\n",
       " 'slim-ratings-tool',\n",
       " 'slim-sa-ner',\n",
       " 'slim-sa-ner-phi-3-gguf',\n",
       " 'slim-sa-ner-phi-3-ov',\n",
       " 'slim-sa-ner-tool',\n",
       " 'slim-sentiment-onnx',\n",
       " 'slim-sentiment-ov',\n",
       " 'slim-sentiment-tool',\n",
       " 'slim-sql-onnx',\n",
       " 'slim-sql-ov',\n",
       " 'slim-sql-tool',\n",
       " 'slim-summary',\n",
       " 'slim-summary-phi-3-gguf',\n",
       " 'slim-summary-phi-3-onnx',\n",
       " 'slim-summary-phi-3-ov',\n",
       " 'slim-summary-tiny',\n",
       " 'slim-summary-tiny-onnx',\n",
       " 'slim-summary-tiny-ov',\n",
       " 'slim-summary-tool',\n",
       " 'slim-tags-3b',\n",
       " 'slim-tags-3b-tool',\n",
       " 'slim-tags-onnx',\n",
       " 'slim-tags-ov',\n",
       " 'slim-tags-tool',\n",
       " 'slim-topics-onnx',\n",
       " 'slim-topics-ov',\n",
       " 'slim-topics-tool',\n",
       " 'slim-xsum',\n",
       " 'slim-xsum-phi-3-gguf',\n",
       " 'slim-xsum-phi-3-ov',\n",
       " 'slim-xsum-tool',\n",
       " 'stablelm-2-zephyr-1_6b-ov',\n",
       " 'stablelm-zephyr-3b-ov',\n",
       " 'summarize-medium',\n",
       " 'summarize-xlarge',\n",
       " 'teknium-open-hermes-2.5-mistral-ov',\n",
       " 'text-ada-001',\n",
       " 'text-babbage-001',\n",
       " 'text-bison@001',\n",
       " 'text-curie-001',\n",
       " 'text-davinci-003',\n",
       " 'tiny-dolphin-2.8-1.1b-ov',\n",
       " 'tiny-llama-chat-gguf',\n",
       " 'tiny-llama-chat-onnx',\n",
       " 'tiny-llama-chat-ov',\n",
       " 'whisper-cpp-base',\n",
       " 'whisper-cpp-base-english',\n",
       " 'whisper-cpp-tiny-diarize',\n",
       " 'yi-6b-1.5v-chat-ov',\n",
       " 'yi-9b-chat-ov',\n",
       " 'zephyr-mistral-7b-chat-ov']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# LIST OF THE EMBEDDING MODELS AVAILABLE\n",
    "generative_models = ModelCatalog().list_generative_models()\n",
    "gen_model_names = [model['model_name'] for model in generative_models]; gen_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "858b4467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for LLM inference -  bling-phi-3-gguf\n",
      "\n",
      "Contract Name:  0 TriviaQA_0.txt\n",
      "\n",
      "update: llm answer -  Not Found.\n",
      "update: Right answer -  S\n",
      "\n",
      "update: Prompt state saved at:  C:\\Users\\xavie\\llmware_data\\prompt_history\\296c13dc-d7c7-444e-a087-7b7df00314ec\n",
      "\n",
      "update: CSV output for human review -  {'report_name': 'interaction_report_2025-11-12_203431.csv', 'report_fp': 'C:\\\\Users\\\\xavie\\\\llmware_data\\\\prompt_history\\\\interaction_report_2025-11-12_203431.csv', 'results': 1}\n",
      "\n",
      "Contract Name:  1 TriviaQA_1.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m llm_model_name = \u001b[33m\"\u001b[39m\u001b[33mbling-phi-3-gguf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mgenerator\u001b[39m\u001b[34m(llm_model_name, digit)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#   note: prompt_with_resource returns a list of dictionary responses\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#   -- depending upon the size of the source context, it may call the llm several times\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#   -- each dict entry represents 1 call to the LLM\u001b[39;00m\n\u001b[32m     36\u001b[39m \n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#   post processing fact checking\u001b[39;00m\n\u001b[32m     38\u001b[39m answer = dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][digit][\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mupdate: llm answer - \u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mllm_response\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mupdate: Right answer - \u001b[39m\u001b[33m\"\u001b[39m, answer[i])\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# start fresh for next document\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "llm_model_name = \"bling-phi-3-gguf\"\n",
    "generator(llm_model_name, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "09a864d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO MAKE GENERATOR MODEL\n",
    "def generator(llm_model_name, digit):\n",
    "    print(\"Loading model for LLM inference - \", llm_model_name)\n",
    "    query = dataset[\"train\"][digit][\"question\"]\n",
    "    prompter = Prompt().load_model(llm_model_name, temperature=0.0, sample=False)\n",
    "    results = Query(library).semantic_query(query, result_count=80, embedding_distance_threshold=1.0)\n",
    "\n",
    "    # for each document in the library, we will run a query and look at the results\n",
    "\n",
    "    for i, contract in enumerate([1]):\n",
    "        qr = []\n",
    "        if contract != \".DS_Store\":\n",
    "            print(\"\\nContract Name: \", i, contract)\n",
    "\n",
    "            #   we will look through the list of semantic query results, and pull the top results for each file\n",
    "            for j, entries in enumerate(results):\n",
    "                library_fn = entries[\"file_source\"]\n",
    "\n",
    "                if os.sep in library_fn:\n",
    "                    # handles difference in windows file formats vs. mac / linux\n",
    "                    library_fn = library_fn.split(os.sep)[-1]\n",
    "\n",
    "                if library_fn == contract:\n",
    "                   # print(\"Top Retrieval: \", j, entries[\"distance\"], entries[\"text\"])\n",
    "                    qr.append(entries)\n",
    "\n",
    "            #   we will add the query results to the prompt\n",
    "            source = prompter.add_source_query_results(query_results=qr)\n",
    "\n",
    "            #   run the prompt\n",
    "            response = prompter.prompt_with_source(query, prompt_name=\"default_with_context\")\n",
    "\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d24356c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for LLM inference -  bling-phi-3-gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mWARNING: No source added in .add_source_query_results.\u001b[39m\n",
      "\u001b[37mWARNING: No source materials attached to the Prompt. Running prompt_with_source inference without source may lead to unexpected results.\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contract Name:  0 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'llm_response': 'Sinclair Lewis.',\n",
       "  'prompt': 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?',\n",
       "  'evidence': '',\n",
       "  'instruction': 'default_with_context',\n",
       "  'model': 'bling-phi-3-gguf',\n",
       "  'usage': {'input': 30,\n",
       "   'output': 6,\n",
       "   'total': 36,\n",
       "   'metric': 'tokens',\n",
       "   'processing_time': 2.619739055633545},\n",
       "  'time_stamp': '2025-11-12_202951',\n",
       "  'calling_app_ID': '',\n",
       "  'rating': '',\n",
       "  'account_name': 'llmware',\n",
       "  'prompt_id': 0,\n",
       "  'batch_id': 0,\n",
       "  'evidence_metadata': [{'evidence_start_char': 0,\n",
       "    'evidence_stop_char': 0,\n",
       "    'page_num': 'NA',\n",
       "    'source_name': 'NA',\n",
       "    'doc_id': 'NA',\n",
       "    'block_id': 'NA'}]}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generator(llm_model_name, 1); response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e6e3b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sinclair Lewis.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0][\"llm_response\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
