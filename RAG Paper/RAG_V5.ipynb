{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3572572",
   "metadata": {},
   "source": [
    "Version 5 of reproduction of the following paper : Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "\n",
    "What is added from V4 : \n",
    "- Based on LLMware, but applied to TriviaQA --> application to something else so that the learning sticks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9a638",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97de868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.setup import Setup\n",
    "from llmware.status import Status\n",
    "from llmware.models import ModelCatalog\n",
    "from llmware.configs import LLMWareConfig, MilvusConfig\n",
    "\n",
    "import time\n",
    "from llmware.prompts import Prompt, HumanInTheLoop\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "from importlib import util\n",
    "\n",
    "from datasets import Dataset, load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591db95",
   "metadata": {},
   "source": [
    "LIBRARY : import documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e2bffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': Value('string'),\n",
       " 'question_id': Value('string'),\n",
       " 'question_source': Value('string'),\n",
       " 'entity_pages': {'doc_source': List(Value('string')),\n",
       "  'filename': List(Value('string')),\n",
       "  'title': List(Value('string')),\n",
       "  'wiki_context': List(Value('string'))},\n",
       " 'search_results': {'description': List(Value('string')),\n",
       "  'filename': List(Value('string')),\n",
       "  'rank': List(Value('int32')),\n",
       "  'title': List(Value('string')),\n",
       "  'url': List(Value('string')),\n",
       "  'search_context': List(Value('string'))},\n",
       " 'answer': {'aliases': List(Value('string')),\n",
       "  'normalized_aliases': List(Value('string')),\n",
       "  'matched_wiki_entity_name': Value('string'),\n",
       "  'normalized_matched_wiki_entity_name': Value('string'),\n",
       "  'normalized_value': Value('string'),\n",
       "  'type': Value('string'),\n",
       "  'value': Value('string')}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT TRIVIAQA dataset\n",
    "\n",
    "# Load TriviaQA (unfiltered for simplicity)\n",
    "\n",
    "dataset = load_dataset(\"trivia_qa\", \"unfiltered\")\n",
    "\n",
    " \n",
    "\n",
    "# Take first 50 examples for quick testing\n",
    "\n",
    "test_set = dataset[\"train\"].select(range(50))\n",
    "\n",
    " \n",
    "\n",
    "# List of the features of the dataset\n",
    "\n",
    "features = dataset[\"train\"].features; features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c04142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1998 was the Chinese year of which creature?\n",
      "Answer: Tiger\n"
     ]
    }
   ],
   "source": [
    "#PRINT ONE QUESTION AND ANSWER\n",
    "\n",
    "digit = 77\n",
    "\n",
    "example = dataset[\"train\"][digit]\n",
    "\n",
    " \n",
    "\n",
    "# Print the question and its answer\n",
    "\n",
    "print(\"Question:\", example[\"question\"])\n",
    "\n",
    "print(\"Answer:\", example[\"answer\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7637a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION THAT WILL CREATE A LIBRARY\n",
    "\n",
    "def create_library(library_name):\n",
    "\n",
    "    print (f\"\\n > Creating library '{library_name}'...\")\n",
    "\n",
    "    library = Library().create_new_library(library_name)\n",
    "\n",
    "    return library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd04af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Creating library 'RAG_V5_Lib'...\n"
     ]
    }
   ],
   "source": [
    "# CREATE LIBRARY\n",
    "\n",
    "library_name = \"RAG_V5_Lib\"\n",
    "\n",
    "library = create_library(library_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f159b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO PARSE THE TRIVIAQA DATASET AND ADD THEM TO THE LIBRARY\n",
    "\n",
    "def dataset_to_file(dataset):\n",
    "    print(\"\\n > Transferring dataset to file ...\")\n",
    "\n",
    "    # Extract text from dataset (e.g., questions and answers)\n",
    "    texts = []\n",
    "\n",
    "    for i in range(len(dataset[\"train\"])):\n",
    "        question = dataset[\"train\"][\"question\"][i]\n",
    "        answer = dataset[\"train\"][\"answer\"][i]\n",
    "        combined_text = f\"Q: {question}\\nA: {answer}\"\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(\"Data\", exist_ok=True)\n",
    "\n",
    "    # Define the full path to the file\n",
    "    file_path = os.path.join(\"Data\", \"TriviaQA.txt\")\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(texts))\n",
    "        print(f\"File saved as {file_path}\")\n",
    "\n",
    "def parse_files(library, data_path):\n",
    "    print (f\"\\n > Parsing and adding dataset to library ...\")\n",
    "    library.add_files(input_folder_path=data_path, chunk_size=400, max_chunk_size=800, smart_chunking=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0a507eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Transferring dataset to file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO: update:  Duplicate files (skipped): 1\u001b[39m\n",
      "\u001b[37mINFO: update:  Total uploaded: 0\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as Data\\TriviaQA.txt\n",
      "\n",
      " > Parsing and adding dataset to library ...\n"
     ]
    }
   ],
   "source": [
    " #APPLY THE FUNCTION\n",
    "dataset_to_file(dataset)\n",
    "parse_files(library, \"Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56999a",
   "metadata": {},
   "source": [
    "RETRIEVER EMBEDDINGS : Turn text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO MAKE EMBEDDINGS\n",
    "\n",
    "def make_embeddings(embedding_model_name, vector_db):\n",
    "    print(\"Generating Embeddings in {} db - with Model- {}\".format(vector_db, embedding_model_name))\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "    MilvusConfig().set_config(\"lite\", True)\n",
    "    LLMWareConfig().set_vector_db(vector_db)\n",
    "    library.install_new_embedding(embedding_model_name=embedding_model_name, vector_db=vector_db, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8bfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST OF THE EMBEDDING MODELS AVAILABLE\n",
    "embedding_models = ModelCatalog().list_embedding_models()\n",
    "model_names = [model['model_name'] for model in embedding_models]; model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794142a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE EMBEDDINGS\n",
    "embedding_model_name = \"mini-lm-sbert\"\n",
    "vector_db = \"faiss\"\n",
    "make_embeddings(embedding_model_name, vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6310cf4",
   "metadata": {},
   "source": [
    "GENERATOR : PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78184b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO MAKE GENERATOR MODEL\n",
    "def generator(llm_model_name, digit):\n",
    "    print(\"Loading model for LLM inference - \", llm_model_name)\n",
    "    query = dataset[\"train\"][digit][\"question\"]\n",
    "    prompter = Prompt().load_model(llm_model_name, temperature=0.0, sample=False)\n",
    "    results = Query(library).semantic_query(query, result_count=80, embedding_distance_threshold=1.0)\n",
    "\n",
    "    # for each document in the library, we will run a query and look at the results\n",
    "\n",
    "    for i, contract in enumerate(os.listdir(data_path)):\n",
    "        qr = []\n",
    "        if contract != \".DS_Store\":\n",
    "            print(\"\\nContract Name: \", i, contract)\n",
    "\n",
    "            #   we will look through the list of semantic query results, and pull the top results for each file\n",
    "            for j, entries in enumerate(results):\n",
    "                library_fn = entries[\"file_source\"]\n",
    "\n",
    "                if os.sep in library_fn:\n",
    "                    # handles difference in windows file formats vs. mac / linux\n",
    "                    library_fn = library_fn.split(os.sep)[-1]\n",
    "\n",
    "                if library_fn == contract:\n",
    "                    print(\"Top Retrieval: \", j, entries[\"distance\"], entries[\"text\"])\n",
    "                    qr.append(entries)\n",
    "\n",
    "            #   we will add the query results to the prompt\n",
    "            source = prompter.add_source_query_results(query_results=qr)\n",
    "\n",
    "            #   run the prompt\n",
    "            response = prompter.prompt_with_source(query, prompt_name=\"default_with_context\")\n",
    "\n",
    "            #   note: prompt_with_resource returns a list of dictionary responses\n",
    "            #   -- depending upon the size of the source context, it may call the llm several times\n",
    "            #   -- each dict entry represents 1 call to the LLM\n",
    "\n",
    "            #   post processing fact checking\n",
    "            answer = dataset[\"train\"][digit][\"answer\"][\"value\"]\n",
    "\n",
    "            for resp in enumerate(response):\n",
    "                if \"llm_response\" in resp:\n",
    "                    print(\"\\nupdate: llm answer - \", resp[\"llm_response\"])\n",
    "                    print(\"update: Right answer - \", answer[resp])\n",
    "\n",
    "            # start fresh for next document\n",
    "            prompter.clear_source_materials()\n",
    "\n",
    "            # Save jsonl report with full transaction history to /prompt_history folder\n",
    "            print(\"\\nupdate: Prompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n",
    "\n",
    "            prompter.save_state()\n",
    "\n",
    "            # Generate CSV report for easy Human review in Excel\n",
    "            csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "            print(\"\\nupdate: CSV output for human review - \", csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6747375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LIST OF THE EMBEDDING MODELS AVAILABLE\n",
    "generative_models = ModelCatalog().list_generative_models()\n",
    "gen_model_names = [model['model_name'] for model in generative_models]; gen_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56326a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"bling-phi-3-gguf\"\n",
    "generator(llm_model_name, digit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
